{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from htmlTemplates import css, bot_template, user_template\n",
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_docs):\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf,strict=False)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_text = get_pdf_text([\"books\\ml_for_begineer.pdf\"])\n",
    "# print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(pdf_text)\n",
    "# print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "pdf_txt_chunks = get_text_chunks(pdf_text)\n",
    "# print(pdf_txt_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store vector embeddings\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "# print(get_vectorstore(pdf_txt_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_chain(vectorstore):\n",
    "    llm = ChatOpenAI()\n",
    "    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True)\n",
    "    \n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Hard-coded inputs\n",
    "pdf_docs = glob.glob('books/*.pdf')\n",
    "# print(f\"Ask question from these books:{pdf_docs}\")\n",
    "\n",
    "# Process the PDFs and create conversation chain\n",
    "print(\"Processing...\")\n",
    "print()\n",
    "raw_text = get_pdf_text(pdf_docs)\n",
    "text_chunks = get_text_chunks(raw_text)\n",
    "vectorstore = get_vectorstore(text_chunks)\n",
    "conversation = get_conversation_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scenario of intelligent machines turning on humans in a struggle of the fittest is a common theme in discussions about artificial intelligence and the future. This concept is often depicted in science fiction and raises concerns about the potential risks associated with highly advanced AI systems. However, it's important to remember that this is a speculative scenario and not a foregone conclusion. The development and deployment of AI technologies are guided by ethical considerations and safety measures to prevent such outcomes. The field of AI ethics is actively addressing these concerns to ensure that AI systems are designed and used responsibly.\n"
     ]
    }
   ],
   "source": [
    "# Handle user input\n",
    "query = conversation(\n",
    "    {'question': 'what was my first question?'}\n",
    "               )\n",
    "print(query['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
